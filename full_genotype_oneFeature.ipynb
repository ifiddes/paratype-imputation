{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 53831, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 40112, \n",
      "  \"hb_port\": 52643, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"d39f3762-ee5b-49c0-a895-40b239462d63\", \n",
      "  \"kernel_name\": \"\", \n",
      "  \"shell_port\": 48522, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 50962\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing /cluster/home/cbosworth/.local/share/jupyter/runtime/kernel-596d0d5d-01c8-4577-beb2-0df65ac2e8c1.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import argparse\n",
    "import vcf\n",
    "import itertools\n",
    "import sys\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from collections import *\n",
    "from tools.bio import *\n",
    "from cat.plots import *\n",
    "from phase_lib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args=argparse.Namespace()\n",
    "\n",
    "#args.features=\"onlyStop.features.tsv\"\n",
    "#args.features=\"/hive/users/ifiddes/notch2nl_berkeley_data/imputation_pipeline/H9_NA12878.features.tsv\"\n",
    "args.features=\"/hive/users/ifiddes/notch2nl_berkeley_data/imputation_pipeline/very_reduced.features.tsv\"\n",
    "args.ratio_plot=\"tmp.pdf\"\n",
    "args.inferred_copy_numbers=[4,2,2,2]\n",
    "#args.bam=\"/hive/users/cbosworth/SimonsNormals/secondTry/LP6005442-DNA_B08.sorted.2.bam\"\n",
    "#args.bam=\"H9/E2del68_E2del19N_combined.100kb.sorted.bam\"\n",
    "args.bam=None\n",
    "args.paratype_pseudo=0.01\n",
    "args.read_pseudo=0\n",
    "args.consensus_fasta='/hive/users/cbosworth/refs/notch/notch2_aligned_consensus.fasta'\n",
    "args.pileup_converter='/cluster/home/ifiddes/pileup2base/pileup2base.pl'\n",
    "args.save_pileup=None\n",
    "#args.pileup=None  \n",
    "args.pileup='/hive/users/ifiddes/notch2nl_berkeley_data/imputation_pipeline/H9.pileup.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_C(inferred_copy_numbers, filtered_features):\n",
    "    \"\"\"Construct C based on previous information of counts. Only allow possibilties to be enumerated that match this information\"\"\"\n",
    "\n",
    "    # create map of paralog to inferred total\n",
    "    inferred_copy_numbers = {x: int(y) for x, y in zip(['AB', 'C', 'D', 'N'], inferred_copy_numbers)}\n",
    "    total_copies = sum(inferred_copy_numbers.values())\n",
    "    features_by_paralog = [x.split('_')[1][0] for x in filtered_features.columns]\n",
    "\n",
    "    \n",
    "    #print(inferred_copy_numbers)\n",
    "    #print(total_copies)\n",
    "    print(filtered_features)\n",
    "    print(features_by_paralog)\n",
    "    # for simplicity, replace all A or B with AB\n",
    "    features_by_paralog = [x if x not in ['A', 'B'] else 'AB' for x in features_by_paralog]\n",
    "\n",
    "    # split them into groups, maintaining original positions\n",
    "    feature_groups = defaultdict(list)\n",
    "    for i, f in enumerate(features_by_paralog):\n",
    "        feature_groups[f].append(i)\n",
    "\n",
    "    #print(feature_groups)\n",
    "    # construct all possibilities for each feature group\n",
    "    possibilities = {}\n",
    "    for f, positions in feature_groups.iteritems():\n",
    "        inferred_copy = inferred_copy_numbers[f]\n",
    "        r = np.array([np.array(x) for x in itertools.product(range(inferred_copy + 1), repeat=len(positions))\n",
    "                      if sum(x) == inferred_copy])\n",
    "        possibilities[f] = r\n",
    "\n",
    "    def array_product(a1, a2):\n",
    "        m1,n1 = a1.shape\n",
    "        m2,n2 = a2.shape\n",
    "        out = np.zeros((m1, m2, n1 + n2), dtype=int)\n",
    "        out[:,:,:n1] = a1[:,None,:]\n",
    "        out[:,:,n1:] = a2\n",
    "        out.shape = (m1 * m2, -1)\n",
    "        return out\n",
    " \n",
    "    #print(possibilities)\n",
    "    abc = array_product(possibilities['AB'], possibilities['C'])\n",
    "    abcd = array_product(abc, possibilities['D'])\n",
    "    abcdn = array_product(abcd, possibilities['N'])\n",
    "\n",
    "    # finally, rearrange the columns to reflect the original positioning\n",
    "    order = feature_groups['AB'] + feature_groups['C'] + feature_groups['D'] + feature_groups['N']\n",
    "    i = np.argsort(order)\n",
    "    ordered = abcdn[:,i]\n",
    "    return ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, seq = read_fasta(args.consensus_fasta, None).next()\n",
    "\n",
    "if args.bam is not None:\n",
    "    pileup_recs = make_pileup(args.bam)\n",
    "    df = convert_pileup(pileup_recs, args.pileup_converter)\n",
    "    data = parse_converted_pileup(df, seq)\n",
    "    if args.save_pileup is not None:\n",
    "        data.to_csv(args.pileup, sep='\\t')\n",
    "else:\n",
    "    data = pd.read_csv(args.pileup, sep='\\t', index_col=0)\n",
    "\n",
    "features = pd.read_csv(args.features, sep='\\t', index_col=0)\n",
    "# find shared positions in case data is missing some\n",
    "positions = set(features.index) & set(data['loc'])\n",
    "filtered_data = data[data['loc'].isin(positions)]\n",
    "# filter features too\n",
    "filtered_features = features[features.index.isin(positions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#C = construct_C(args.inferred_copy_numbers, filtered_features)\n",
    "#C=construct_C(args.inferred_copy_numbers,features)\n",
    "import cPickle as pickle\n",
    "C = pickle.load(open('very_reduced.features.precomputed_C_4_2_2_2.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01,  0.01,  0.01,  1.  ,  0.01,  1.  ,  0.01,  0.01,  0.01,  0.01])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create map of paralog to inferred total\n",
    "inferred_copy_numbers = {x: int(y) for x, y in zip(['AB', 'C', 'D', 'N'], args.inferred_copy_numbers)}\n",
    "total_copies = sum(inferred_copy_numbers.values())\n",
    "features_by_paralog = [x.split('_')[1][0] for x in filtered_features.columns]\n",
    "features_by_paralog = [x if x not in ['A', 'B'] else 'AB' for x in features_by_paralog]\n",
    "\n",
    "feature_groups = defaultdict(list)\n",
    "for i, f in enumerate(features_by_paralog):\n",
    "    feature_groups[f].append(i)\n",
    "feature_groups\n",
    "\n",
    "ambig=[]\n",
    "ab=np.sum(filtered_features[feature_groups['AB']],axis=1)\n",
    "c=np.sum(filtered_features[feature_groups['C']],axis=1)\n",
    "d=np.sum(filtered_features[feature_groups['D']],axis=1)\n",
    "n=np.sum(filtered_features[feature_groups['N']],axis=1)\n",
    "\n",
    "#ab>0 and c==0\n",
    "\n",
    "notAmbig=((ab>0) & (c==0) & (d==0) & (n==0)) | ((ab==0) & (c>0) & (d==0) & (n==0)) | ((ab==0) & (c==0) & (d>0) & (n==0)) | ((ab==0) & (c==0) & (d==0) & (n>0))\n",
    "ambig=~(((ab>0) & (c==0) & (d==0) & (n==0)) | ((ab==0) & (c>0) & (d==0) & (n==0)) | ((ab==0) & (c==0) & (d>0) & (n==0)) | ((ab==0) & (c==0) & (d==0) & (n>0)))\n",
    "\n",
    "args.paratype_pseudo=np.asarray(1-0.99*notAmbig)\n",
    "args.paratype_pseudo[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.010000000000000009: 1016, 1.0: 235})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Counter\n",
    "from collections import *\n",
    "Counter(args.paratype_pseudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ct = C.T\n",
    "\n",
    "num = np.dot(filtered_features, Ct)\n",
    "denom = np.sum(Ct, axis=0)[0]\n",
    "\n",
    "#(args.paratype_pseudo + num.T).T.shape\n",
    "denom=[denom]*len(args.paratype_pseudo)+args.paratype_pseudo\n",
    "#(args.paratype_pseudo+num.T).T.shape\n",
    "#( (2.0*args.paratype_pseudo) + denom).shape\n",
    "\n",
    "S = (args.paratype_pseudo + num.T) / ( (2.0*args.paratype_pseudo) + denom)\n",
    "\n",
    "S_log = np.log(S)\n",
    "S_inv = np.log(1 - S)\n",
    "\n",
    "# M is the number of alt reads, N is the number of ref reads\n",
    "M = 1.0 * filtered_data.alt_count\n",
    "N = 1.0 * filtered_data.ref_count\n",
    "\n",
    "#S_log.shape\n",
    "R = (np.dot(M + args.read_pseudo, S_log.T) + np.dot(N + args.read_pseudo, S_inv.T)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_map = {i: x for i, x in enumerate(R)}\n",
    "best_index, score = sorted(R_map.iteritems(), key=lambda x: x[1])[-1]\n",
    "best_haps = C[best_index]\n",
    "\n",
    "best_s = S[best_index]\n",
    "expected_alt = np.multiply(best_s, filtered_data['coverage'])\n",
    "expected_ref = filtered_data['coverage'] - expected_alt\n",
    "actual_alt = filtered_data['alt_count']\n",
    "actual_ref = filtered_data['ref_count']\n",
    "n = filtered_data['coverage']\n",
    "deviance = (expected_alt - actual_alt) / (np.sqrt(n * best_s * (1 - best_s)))\n",
    "variance = sum(np.multiply(deviance, deviance) ) / len(filtered_data['coverage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = zip(deviance.index, list(deviance))\n",
    "q = sorted(q,key=lambda x: x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log odds: -1843316.49056\n",
      "variance: 3782.94476791\n",
      "5 lowest variance sites: \n",
      "46024: -1549.10993118\n",
      "50119: -905.841481676\n",
      "27923: -653.79968399\n",
      "86723: -574.888438265\n",
      "21662: -385.349471998\n",
      "\n",
      "results: \n",
      "CHM1_N_c3: 1\n",
      "NA12878_D2_c6: 2\n",
      "NA24385_N1_c3: 1\n",
      "NA24385_C1_c2: 1\n",
      "H9_A1_c8: 1\n",
      "H9_B1_c1: 1\n",
      "H9_B2_c3: 1\n",
      "H9_B3_c7: 1\n",
      "H9_C1_c4: 1\n",
      "top 10 hits:\n",
      "CHM1_A_c5 CHM1_B_c4 CHM1_N_c3 NA12878_A1_c7 NA12878_B1_c3 NA12878_B2_c5 NA12878_D2_c6 NA24385_B1_c4 NA24385_B2_c5 NA24385_A1_c0 NA24385_A2_c6 NA24385_N1_c3 NA24385_C1_c2 H9_A1_c8 H9_B1_c1 H9_B2_c3 H9_B3_c7 H9_C1_c4\n",
      "1: [array([0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]), 184, -1843316.4905618769]\n",
      "2: [array([0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1]), 9139, -1844190.1227900477]\n",
      "3: [array([0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]), 9364, -1845158.2444901722]\n",
      "4: [array([0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]), 9130, -1845742.1339456602]\n",
      "5: [array([0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1]), 679, -1846659.3139528902]\n",
      "6: [array([0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1]), 9355, -1848915.5366652878]\n",
      "7: [array([1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1]), 12415, -1852272.2135881763]\n",
      "8: [array([1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1]), 12334, -1856251.5924962016]\n",
      "9: [array([1, 0, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1]), 12640, -1858400.7827272783]\n",
      "10: [array([0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1]), 9058, -1859630.6096949703]\n"
     ]
    }
   ],
   "source": [
    "print 'log odds: {}'.format(score)\n",
    "print 'variance: {}'.format(variance)\n",
    "print '5 lowest variance sites: '\n",
    "for pos, var in q[:5]:\n",
    "    print '{}: {}'.format(pos, var)\n",
    "print ''\n",
    "print 'results: '\n",
    "for x, y in zip(filtered_features.columns, best_haps):\n",
    "    if y > 0:\n",
    "        print '{}: {}'.format(x, y)\n",
    "\n",
    "print 'top 10 hits:'\n",
    "ordered = sorted(R_map.iteritems(), key=lambda x: x[1])[-10:][::-1]\n",
    "print ' '.join(features.columns)\n",
    "for i, x in enumerate([[C[pos], pos, val] for pos, val in ordered], 1):\n",
    "    print '{}: {}'.format(i, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"stdin_port\": 58075, \n",
      "  \"ip\": \"127.0.0.1\", \n",
      "  \"control_port\": 39227, \n",
      "  \"hb_port\": 60785, \n",
      "  \"signature_scheme\": \"hmac-sha256\", \n",
      "  \"key\": \"303b584f-317c-4c17-9a35-e4b0cc3cbe57\", \n",
      "  \"kernel_name\": \"\", \n",
      "  \"shell_port\": 38236, \n",
      "  \"transport\": \"tcp\", \n",
      "  \"iopub_port\": 58976\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> ipython <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> ipython <app> --existing /cluster/home/ifiddes/.local/share/jupyter/runtime/kernel-0e062627-c986-404b-bc23-c03bd04ca292.json \n",
      "or even just:\n",
      "    $> ipython <app> --existing \n",
      "if this is the most recent IPython session you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1251, 16380)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1251,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 16380)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
